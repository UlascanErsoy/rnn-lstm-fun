{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 characters found!\n"
     ]
    }
   ],
   "source": [
    "poems_text = open(\"yunus_emre.txt\",\"r\").read().lower()\n",
    "\n",
    "all_letters = \"\".join(sorted(list(set(poems_text))))\n",
    "\n",
    "print(f\"{len(all_letters)} characters found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_to_index(letter: str):\n",
    "    \"\"\"converts letter to an index\n",
    "    \"\"\"\n",
    "    return all_letters.find(letter.lower())\n",
    "\n",
    "def letter_to_tensor(letter: str):\n",
    "    tensor = torch.zeros(1, len(all_letters))\n",
    "    tensor[0][letter_to_index(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def line_to_tensor(line):\n",
    "    tensor = torch.zeros(len(line),1,len(all_letters),dtype=torch.float32)\n",
    "    for idx, letter in tqdm(enumerate(line)):\n",
    "        tensor[idx][0][letter_to_index(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size:int,\n",
    "                        hidden_size:int,\n",
    "                        hidden_layers: int = 1):\n",
    "        \"\"\"Initialize an LSTM \n",
    "        for training purposes\n",
    "        \"\"\"\n",
    "        super(LSTM,self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden = None\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=hidden_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.h2o = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        if self.hidden is None:\n",
    "            output, self.hidden = self.lstm(input)\n",
    "        else:\n",
    "            output, self.hidden = self.lstm(input,self.hidden)\n",
    "\n",
    "        self.hidden = tuple(item.detach() for item in self.hidden)\n",
    "        output = self.h2o(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, inputs, optimizer,lr=0.005):\n",
    "    \n",
    "    \n",
    "    rnn.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    losses = 0\n",
    "    clipping_value = 1 # arbitrary value of your choosing\n",
    "    rnn.hidden = None\n",
    "    for idx in range(inputs.shape[0]-1):\n",
    "        output = rnn(inputs[idx])\n",
    "        loss = loss_fn(torch.flatten(output), torch.flatten(inputs[idx+1]))\n",
    "        losses += loss\n",
    "    \n",
    "    #optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #optimizer.step()\n",
    "    torch.nn.utils.clip_grad_norm_(rnn.parameters(), clipping_value)\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-lr)\n",
    "\n",
    "    return losses / (inputs.shape[0])\n",
    "\n",
    "def train_other(rnn, \n",
    "                inputs, \n",
    "                optimizer, \n",
    "                min_seq_size=20, \n",
    "                max_seq_size=60):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    losses = 0\n",
    "    rnn.train()\n",
    "    pbar = tqdm(range(len(inputs) - max_seq_size))\n",
    "    for idx in pbar:\n",
    "        rnn.zero_grad()\n",
    "        seq_len = random.randint(min_seq_size,max_seq_size)\n",
    "        rnn.hidden = None\n",
    "        input, target = inputs[idx:idx+seq_len], inputs[idx+1:idx+seq_len+1]\n",
    "        output = rnn(input)\n",
    "\n",
    "        loss = loss_fn(torch.flatten(output), torch.flatten(target))\n",
    "        #loss = loss_fn(output, target)\n",
    "        losses += loss\n",
    "        loss.backward()\n",
    "        #optimizer.step()\n",
    "        for p in rnn.parameters():\n",
    "            p.data.add_(p.grad.data, alpha=-0.005)\n",
    "\n",
    "        if idx % 1000 == 0:\n",
    "            pbar.set_description(f\"Epoch: {idx}: {losses/idx:.2f}\")\n",
    "  \n",
    "    return losses / idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114972it [00:00, 280589.15it/s]\n"
     ]
    }
   ],
   "source": [
    "input = line_to_tensor(poems_text).to(device)\n",
    "\n",
    "rnn = LSTM(\n",
    "    input_size=len(all_letters),\n",
    "    hidden_size=512,\n",
    "    hidden_layers=2\n",
    ").to(device)\n",
    "rnn.load_state_dict(torch.load(\"rnn.bin\"))\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), amsgrad=True, lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch = 0\n",
    "# while True:\n",
    "\n",
    "#     loss = train_other(rnn, input, optimizer)\n",
    "\n",
    "#     #print(f\"Epoch {epoch} | Loss: {loss}\")\n",
    "#     epoch += 1\n",
    "# while True:\n",
    "\n",
    "#     pbar = tqdm(range(10000))\n",
    "#     batch_size = random.randint(30,100) #100\n",
    "#     batch_count = input.shape[0]//batch_size\n",
    "#     loss_epoch = []\n",
    "    \n",
    "#     for epoch in pbar:\n",
    "\n",
    "#         batch = int(random.random() * batch_count)\n",
    "#         rnn.zero_grad()\n",
    "#         inps = input[batch*batch_size: batch*batch_size + batch_size]\n",
    "#         loss = train(rnn, inps, optimizer,lr=0.001)\n",
    "#         loss_epoch.append(float(loss))\n",
    "#         pbar.set_description(f\"Epoch {epoch} | Batch Size: {batch_size} | Loss: {sum(loss_epoch)/len(loss_epoch):.2f}\")\n",
    "\n",
    "#         if epoch % 1000 == 0:\n",
    "#         #         print(f\"Epoch {epoch} Avg loss: {sum(loss_epoch)/len(loss_epoch):.2f}\")\n",
    "#                 loss_epoch = []\n",
    "\n",
    "#     print(f\"Epoch {epoch} Avg loss: {sum(loss_epoch)/len(loss_epoch):.2f}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn.state_dict(), \"rnn.bin\")\n",
    "#rnn.load_state_dict(torch.load(\"rnn_2layer_512_softmax.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bir ben variim canım allarım allah sana beni gelir\n",
      "\n",
      "\n",
      "\n",
      "aşkın beni gelir\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ene gelir bir gönül kalam ol ya kalın gönül gördüm elim bu yaşk ile derviş gelir\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ene gelir bir dem bir gönül eyle derviş yana gelir\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rnn.eval()\n",
    "\n",
    "def tensor_to_char(tensor):\n",
    "    return all_letters[int((tensor == 1).nonzero(as_tuple=True)[0])]\n",
    "\n",
    "ch = \"bir ben var\"\n",
    "for c in ch:\n",
    "    output = letter_to_tensor(c).to(device)\n",
    "    output = rnn(output)\n",
    "    #print(c,end=\"\")\n",
    "\n",
    "print(ch,end=\"\")\n",
    "rnn.hidden = None\n",
    "with torch.no_grad():\n",
    "    for idx in range(200):\n",
    "\n",
    "        output = rnn(output)\n",
    "        output_dist = output.data.view(-1).div(0.9).exp()\n",
    "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
    "        #top_i = int(output.argmax())#\n",
    "        print(all_letters[top_i],end=\"\")\n",
    "        output = letter_to_tensor(all_letters[top_i]).to(device)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
